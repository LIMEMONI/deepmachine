{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # 1채널 입력을 32채널로 변환하는 3x3 컨볼루션 레이어\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # 32채널 입력을 64채널로 변환하는 3x3 컨볼루션 레이어\n",
    "        self.dropout1 = nn.Dropout(0.25)  # 25%의 드롭아웃 비율을 가지는 드롭아웃 레이어\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 50%의 드롭아웃 비율을 가지는 드롭아웃 레이어\n",
    "        self.fc1 = nn.Linear(9216, 128)  # 9216개의 입력을 128개의 출력으로 변환하는 fully connected 레이어\n",
    "        self.fc2 = nn.Linear(128, 10)  # 128개의 입력을 10개의 출력으로 변환하는 fully connected 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # 첫 번째 컨볼루션 레이어를 통과한 결과\n",
    "        x = F.relu(x)  # ReLU 활성화 함수를 적용한 결과\n",
    "        x = self.conv2(x)  # 두 번째 컨볼루션 레이어를 통과한 결과\n",
    "        x = F.relu(x)  # ReLU 활성화 함수를 적용한 결과\n",
    "        x = F.max_pool2d(x, 2)  # 2x2 최대 풀링 레이어를 통과한 결과\n",
    "        x = self.dropout1(x)  # 첫 번째 드롭아웃 레이어를 통과한 결과\n",
    "        x = torch.flatten(x, 1)  # 1차원으로 펼친 결과\n",
    "        x = self.fc1(x)  # 첫 번째 fully connected 레이어를 통과한 결과\n",
    "        x = F.relu(x)  # ReLU 활성화 함수를 적용한 결과\n",
    "        x = self.dropout2(x)  # 두 번째 드롭아웃 레이어를 통과한 결과\n",
    "        x = self.fc2(x)  # 두 번째 fully connected 레이어를 통과한 결과\n",
    "        output = F.log_softmax(x, dim=1)  # 로그-소프트맥스 활성화 함수를 적용한 결과\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)  # 데이터와 타겟을 디바이스로 이동\n",
    "        optimizer.zero_grad()  # 옵티마이저의 그래디언트 초기화\n",
    "        output = model(data)  # 모델에 데이터를 입력하여 출력값 계산\n",
    "        loss = F.nll_loss(output, target)  # 출력값과 타겟을 비교하여 손실 계산\n",
    "        loss.backward()  # 손실에 대한 그래디언트 계산\n",
    "        optimizer.step()  # 옵티마이저를 사용하여 모델 파라미터 업데이트\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))  # 현재 배치의 학습 진행 상황 출력\n",
    "            if args.dry_run:\n",
    "                break  # dry_run이 True인 경우 학습을 한 번만 수행하도록 중단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    test_loss = 0  # 테스트 손실 초기화\n",
    "    correct = 0  # 정확한 예측 수 초기화\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for data, target in test_loader:  # 테스트 데이터로더에서 데이터와 타겟 가져오기\n",
    "            data, target = data.to(device), target.to(device)  # 데이터와 타겟을 디바이스로 이동\n",
    "            output = model(data)  # 모델에 데이터 입력하여 출력 얻기\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # 배치 손실 합산\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 최대 로그 확률의 인덱스 가져오기\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # 정확한 예측 수 합산\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # 평균 테스트 손실 계산\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))  # 테스트 결과 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')  # PyTorch MNIST 예제에 대한 설명을 포함하는 인수 파서 생성\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')  # 훈련에 사용할 입력 배치 크기를 설정하는 인수 추가\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')  # 테스트에 사용할 입력 배치 크기를 설정하는 인수 추가\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')  # 훈련할 에포크 수를 설정하는 인수 추가\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')  # 학습률을 설정하는 인수 추가\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')  # 학습률 스텝 감마를 설정하는 인수 추가\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')  # CUDA 훈련을 비활성화하는 인수 추가\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')  # macOS GPU 훈련을 비활성화하는 인수 추가\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')  # 단일 패스를 빠르게 확인하는 인수 추가\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')  # 랜덤 시드를 설정하는 인수 추가\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')  # 훈련 상태를 기록하기 전에 기다려야 할 배치 수를 설정하는 인수 추가\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')  # 현재 모델을 저장하는 인수 추가\n",
    "    args = parser.parse_args()  # 명령행 인수를 파싱하여 args에 저장\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()  # CUDA를 사용할 수 있는지 확인하여 use_cuda에 저장\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()  # MPS를 사용할 수 있는지 확인하여 use_mps에 저장\n",
    "\n",
    "    torch.manual_seed(args.seed)  # 랜덤 시드 설정\n",
    "\n",
    "    if use_cuda:  # CUDA를 사용할 수 있는 경우\n",
    "        device = torch.device(\"cuda\")  # device를 CUDA로 설정\n",
    "    elif use_mps:  # MPS를 사용할 수 있는 경우\n",
    "        device = torch.device(\"mps\")  # device를 MPS로 설정\n",
    "    else:  # CUDA 및 MPS를 사용할 수 없는 경우\n",
    "        device = torch.device(\"cpu\")  # device를 CPU로 설정\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}  # 훈련 데이터로더에 대한 인수 설정\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}  # 테스트 데이터로더에 대한 인수 설정\n",
    "    if use_cuda:  # CUDA를 사용할 수 있는 경우\n",
    "        cuda_kwargs = {'num_workers': 1, # 사용 cpu 대수\n",
    "                       'pin_memory': True, # 메모리 잡기술로 속도 업\n",
    "                       'shuffle': True}  # epoch마다 데이터 섞을지 여부\n",
    "        train_kwargs.update(cuda_kwargs)  # 훈련 데이터로더에 CUDA 관련 인수 추가\n",
    "        test_kwargs.update(cuda_kwargs)  # 테스트 데이터로더에 CUDA 관련 인수 추가\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    transform=transforms.Compose([  # 데이터 변환을 위한 Compose 객체 생성\n",
    "        transforms.ToTensor(),  # 이미지를 텐서로 변환하는 변환 추가\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # 이미지를 정규화하는 변환 추가 (평균, 표준편차 값)\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)  # 훈련 데이터셋 생성\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)  # 테스트 데이터셋 생성\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)  # 훈련 데이터로더 생성\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)  # 테스트 데이터로더 생성\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = Net().to(device)  # 모델 생성 및 device에 할당\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)  # 옵티마이저 생성\n",
    "\n",
    "\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)  # 스케줄러 생성\n",
    "    for epoch in range(1, args.epochs + 1):  # 에포크 반복\n",
    "        train(args, model, device, train_loader, optimizer, epoch)  # 훈련 함수 호출\n",
    "        test(model, device, test_loader)  # 테스트 함수 호출\n",
    "        scheduler.step()  # 스케줄러 갱신\n",
    "\n",
    "    if args.save_model:  # 모델 저장이 설정된 경우\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")  # 모델 저장"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
